{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nedOeJNPHiir",
        "outputId": "02131b08-ff5b-4f1b-87d4-5a256610448d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Robustness-of-Bayesian-Neural-Networks-against-White-Box-Attacks'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 102 (delta 26), reused 69 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (102/102), 924.32 KiB | 7.90 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Soumik-Roy/Robustness-of-Bayesian-Neural-Networks-against-White-Box-Attacks.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -a Robustness-of-Bayesian-Neural-Networks-against-White-Box-Attacks/. ./"
      ],
      "metadata": {
        "id": "8OkUg1GQRnIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ./Robustness-of-Bayesian-Neural-Networks-against-White-Box-Attacks"
      ],
      "metadata": {
        "id": "PhYL5Q1NSA6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCfxyKiVoYEg",
        "outputId": "7a73f324-4a60-4d09-d358-6009997d4677"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchattacks\n",
            "  Downloading torchattacks-3.2.6-py3-none-any.whl (105 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 30 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 40 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 51 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 61 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 71 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 81 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 92 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 102 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 105 kB 9.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: torchattacks\n",
            "Successfully installed torchattacks-3.2.6\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "from models.BayesianModels.Bayesian3Conv3FC import BBB3Conv3FC\n",
        "from models.BayesianModels.BayesianAlexNet import BBBAlexNet\n",
        "from models.BayesianModels.BayesianLeNet import BBBLeNet\n",
        "from models.NonBayesianModels.AlexNet import AlexNet\n",
        "from models.NonBayesianModels.LeNet import LeNet\n",
        "from models.NonBayesianModels.ThreeConvThreeFC import ThreeConvThreeFC\n",
        "import data\n",
        "import utils\n",
        "import metrics\n",
        "import config_frequentist as cfg\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "!pip install torchattacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6B5SZPIfra1"
      },
      "outputs": [],
      "source": [
        "from torchattacks import PGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8ga-kdnQ-Dt"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdZ9-FhgR7oX"
      },
      "outputs": [],
      "source": [
        "def getModel(net_type, inputs, outputs):\n",
        "    if (net_type == 'lenet'):\n",
        "        return LeNet(outputs, inputs)\n",
        "    elif (net_type == 'alexnet'):\n",
        "        return AlexNet(outputs, inputs)\n",
        "    elif (net_type == '3conv3fc'):\n",
        "        return ThreeConvThreeFC(outputs, inputs)\n",
        "    else:\n",
        "        raise ValueError('Network should be either [LeNet / AlexNet / 3Conv3FC')\n",
        "\n",
        "\n",
        "def test_model(net, criterion, test_loader):\n",
        "    valid_loss = 0.0\n",
        "    net.eval()\n",
        "    accs = []\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = net(data)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        accs.append(metrics.acc(output.detach(), target))\n",
        "    return valid_loss, np.mean(accs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6cYKh0hVaRJ"
      },
      "outputs": [],
      "source": [
        "net_type = \"lenet\"\n",
        "dataset = \"MNIST\"\n",
        "n_epochs = cfg.n_epochs\n",
        "lr = cfg.lr\n",
        "num_workers = cfg.num_workers\n",
        "valid_size = cfg.valid_size\n",
        "batch_size = cfg.batch_size\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNTT4qZeXFEA"
      },
      "outputs": [],
      "source": [
        "!mkdir adv_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FXK8fBwSRx8"
      },
      "outputs": [],
      "source": [
        "trainset, testset, inputs, outputs = data.getDataset(dataset)\n",
        "train_loader, valid_loader, test_loader = data.getDataloader(\n",
        "        trainset, testset, valid_size, batch_size, num_workers)\n",
        "model = getModel(net_type, inputs, outputs).to(device)\n",
        "\n",
        "ckpt_dir = f'checkpoints/{dataset}/frequentist'\n",
        "ckpt_name = f'checkpoints/{dataset}/frequentist/model_{net_type}.pt'\n",
        "model.load_state_dict(torch.load(ckpt_name))\n",
        "model = model.eval().cuda()\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "lr_sched = lr_scheduler.ReduceLROnPlateau(optimizer, patience=6, verbose=True)\n",
        "atk = PGD(model, eps=8/255, alpha=2/255, steps=7)\n",
        "atk.set_return_type('int') # Save as integer.\n",
        "atk.save(data_loader=test_loader, save_path=\"adv_data/mnist_pgd.pt\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfmtfJRMWWVV",
        "outputId": "9d49b408-522b-4f12-dd2e-bbbf85b28cae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = test_model(model, criterion, test_loader)\n",
        "\n",
        "test_loss = test_loss/len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bsRC1pBbQ5O",
        "outputId": "aaef62a8-0ec0-4f4c-8b22-a10505f94422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2484176315844059\n",
            "0.95185546875\n"
          ]
        }
      ],
      "source": [
        "print(test_loss)\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzOcLuHBcSVv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "adv_images, adv_labels = torch.load(\"adv_data/mnist_pgd.pt\")\n",
        "adv_data = TensorDataset(adv_images.float()/255, adv_labels)\n",
        "adv_loader = DataLoader(adv_data, batch_size=cfg.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5vyWn8gciaT",
        "outputId": "7cf00e20-d71f-4e7b-965e-cbbf2f1bbe1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.514749061203003\n",
            "0.20703125\n"
          ]
        }
      ],
      "source": [
        "adv_loss, adv_acc = test_model(model, criterion, adv_loader)\n",
        "\n",
        "adv_loss = adv_loss/len(adv_loader.dataset)\n",
        "print(adv_loss)\n",
        "print(adv_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3SwX6Jrrd9U"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "def BPGD(model, criterion, len, image,labels, eps=0.3, alpha=2/255, iters=40,num_ens=1) :\n",
        "    # images = images.to(device)\n",
        "    # labels = labels.to(device)\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "        \n",
        "    # ori_images = images.data\n",
        "    training_loss = 0.0\n",
        "    accs = []\n",
        "    kl_list = []\n",
        "    for i in range(iters) :  \n",
        "          inputs= image.to(device)\n",
        "          labels=labels.to(device)\n",
        "          outputs = torch.zeros(inputs.shape[0], model.num_classes, num_ens).to(device)\n",
        "          inputs.requires_grad = True\n",
        "          kl = 0.0\n",
        "          for j in range(num_ens):\n",
        "              net_out, _kl = model(inputs)\n",
        "              kl += _kl\n",
        "              outputs[:, :, j] = F.log_softmax(net_out, dim=1)\n",
        "          \n",
        "          kl = kl / num_ens\n",
        "          kl_list.append(kl.item())\n",
        "          log_outputs = utils.logmeanexp(outputs, dim=2)\n",
        "\n",
        "          beta = 1/ len\n",
        "          loss = criterion(log_outputs, labels, kl, beta)\n",
        "          loss.backward()\n",
        "          inputs = inputs + alpha*inputs.grad.sign()\n",
        "          eta = torch.clamp(adv_images - inputs, min=-eps, max=eps)\n",
        "          inputs = torch.clamp(inputs + eta, min=0, max=1).detach_()\n",
        "    return inputs\n",
        "\n",
        "    # for i in range(iters) :    \n",
        "    #     images.requires_grad = True\n",
        "    #     outputs = model(images)\n",
        "\n",
        "    #     model.zero_grad()\n",
        "    #     cost = loss(outputs, labels).to(device)\n",
        "    #     cost.backward()\n",
        "\n",
        "    #     adv_images = images + alpha*images.grad.sign()\n",
        "    #     eta = torch.clamp(adv_images - ori_images, min=-eps, max=eps)\n",
        "    #     images = torch.clamp(ori_images + eta, min=0, max=1).detach_()\n",
        "            \n",
        "    # return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McxIL-Y3nfFt",
        "outputId": "bb703bcb-10ac-4278-ee03-598dc39e95c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "import config_bayesian as cfg2\n",
        "\n",
        "def getModel(net_type, inputs, outputs, priors, layer_type, activation_type):\n",
        "    if (net_type == 'lenet'):\n",
        "        return BBBLeNet(outputs, inputs, priors, layer_type, activation_type)\n",
        "    elif (net_type == 'alexnet'):\n",
        "        return BBBAlexNet(outputs, inputs, priors, layer_type, activation_type)\n",
        "    elif (net_type == '3conv3fc'):\n",
        "        return BBB3Conv3FC(outputs, inputs, priors, layer_type, activation_type)\n",
        "    else:\n",
        "        raise ValueError('Network should be either [LeNet / AlexNet / 3Conv3FC')\n",
        "net_type = \"lenet\"\n",
        "dataset = \"MNIST\"\n",
        "layer_type = cfg2.layer_type\n",
        "activation_type = cfg2.activation_type\n",
        "priors = cfg2.priors\n",
        "criterion = metrics.ELBO(len(test_loader)).to(device)\n",
        "\n",
        "train_ens = cfg2.train_ens\n",
        "valid_ens = cfg2.valid_ens\n",
        "n_epochs = cfg2.n_epochs\n",
        "lr_start = cfg2.lr_start\n",
        "num_workers = cfg2.num_workers\n",
        "valid_size = cfg2.valid_size\n",
        "batch_size = cfg2.batch_size\n",
        "beta_type = cfg2.beta_type\n",
        "\n",
        "trainset, testset, inputs, outputs = data.getDataset(dataset)\n",
        "train_loader, valid_loader, test_loader = data.getDataloader(\n",
        "        trainset, testset, valid_size, batch_size, num_workers)\n",
        "\n",
        "model2 = getModel(net_type, inputs, outputs, priors, layer_type, activation_type).to(device)\n",
        "\n",
        "ckpt_dir = f'checkpoints/{dataset}/bayesian'\n",
        "ckpt_name = f'checkpoints/{dataset}/bayesian/model_{net_type}_{layer_type}_{activation_type}.pt'\n",
        "\n",
        "model2.load_state_dict(torch.load(ckpt_name))\n",
        "model2 = model2.eval().cuda()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04_th1Qyvho-"
      },
      "outputs": [],
      "source": [
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "\n",
        "for images, labels in test_loader:\n",
        "    model2.train()\n",
        "    btest_loss=0.0\n",
        "    btest_accs=[]\n",
        "    adv_images = BPGD(model2,criterion,len(test_loader),images,labels, eps=8/255, alpha=2/255, iters=100)\n",
        "    labels = labels.to(device)\n",
        "    outputs = torch.zeros(adv_images.shape[0], model2.num_classes, 1).to(device)\n",
        "    kl = 0.0\n",
        "    for j in range(1):\n",
        "        net_out, _kl = model2(adv_images)\n",
        "        kl += _kl\n",
        "        outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n",
        "\n",
        "    log_outputs = utils.logmeanexp(outputs, dim=2)\n",
        "\n",
        "    criterion = metrics.ELBO(len(test_loader)).to(device)\n",
        "    \n",
        "    beta = 1/ len(test_loader)\n",
        "    btest_loss += criterion(log_outputs, labels, kl, beta).item()\n",
        "    btest_accs.append(metrics.acc(log_outputs, labels))\n",
        "\n",
        "print( btest_loss/len(test_loader), np.mean(btest_accs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz0bOUWGRkKi",
        "outputId": "8f1edf9b-ab01-4397-8b72-0388ab1245cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file 'testadv_b': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python testadv_b --dataset MNIST --attack PGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3p0OkiG2ItP"
      },
      "outputs": [],
      "source": [
        "# !pip install torchattacks\n",
        "import pkg_resources\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "REQUIRED = {\n",
        "  'spacy', 'scikit-learn', 'numpy', 'pandas', 'torch', \n",
        "  'torchattacks', 'matplotlib'\n",
        "}\n",
        "\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = REQUIRED - installed\n",
        "\n",
        "if missing:\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.insert(0, \"BayesianPyTorch\") # to include ../helper_evaluate.py etc.\n",
        "\n",
        "\n",
        "import os \n",
        "# os.chdir(\"/content/\")\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from models.BayesianModels.Bayesian3Conv3FC import BBB3Conv3FC\n",
        "from models.BayesianModels.BayesianAlexNet import BBBAlexNet\n",
        "from models.BayesianModels.BayesianLeNet import BBBLeNet\n",
        "from models.NonBayesianModels.AlexNet import AlexNet\n",
        "from models.NonBayesianModels.LeNet import LeNet\n",
        "from models.NonBayesianModels.ThreeConvThreeFC import ThreeConvThreeFC\n",
        "\n",
        "import os.path\n",
        "import data\n",
        "import utils\n",
        "import metrics\n",
        "\n",
        "import config_bayesian as cfg2\n",
        "import config_frequentist as cfg\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchattacks import PGD, FGSM\n",
        "import gzip,tarfile\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def getBModel(net_type, inputs, outputs, priors, layer_type, activation_type):\n",
        "    if (net_type == 'lenet'):\n",
        "        return BBBLeNet(outputs, inputs, priors, layer_type, activation_type)\n",
        "    elif (net_type == 'alexnet'):\n",
        "        return BBBAlexNet(outputs, inputs, priors, layer_type, activation_type)\n",
        "    elif (net_type == '3conv3fc'):\n",
        "        return BBB3Conv3FC(outputs, inputs, priors, layer_type, activation_type)\n",
        "    else:\n",
        "        raise ValueError('Network should be either [LeNet / AlexNet / 3Conv3FC')\n",
        "\n",
        "def getFModel(net_type, inputs, outputs):\n",
        "    if (net_type == 'lenet'):\n",
        "        return LeNet(outputs, inputs)\n",
        "    elif (net_type == 'alexnet'):\n",
        "        return AlexNet(outputs, inputs)\n",
        "    elif (net_type == '3conv3fc'):\n",
        "        return ThreeConvThreeFC(outputs, inputs)\n",
        "    else:\n",
        "        raise ValueError('Network should be either [LeNet / AlexNet / 3Conv3FC')\n",
        "\n",
        "def validate_model(net, criterion, validloader, num_ens=1, beta_type=0.1):\n",
        "    \"\"\"Calculate ensemble accuracy and NLL Loss\"\"\"\n",
        "    net.train()\n",
        "    valid_loss = 0.0\n",
        "    accs = []\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(validloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n",
        "        kl = 0.0\n",
        "        for j in range(num_ens):\n",
        "            net_out, _kl = net(inputs)\n",
        "            kl += _kl\n",
        "            outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n",
        "\n",
        "        log_outputs = utils.logmeanexp(outputs, dim=2)\n",
        "\n",
        "        beta = 1/len(validloader)\n",
        "        valid_loss += criterion(log_outputs, labels, kl, beta).item()\n",
        "        accs.append(metrics.acc(log_outputs, labels))\n",
        "\n",
        "    return valid_loss/len(validloader), np.mean(accs)\n",
        "\n",
        "def test_model(net, criterion, test_loader):\n",
        "    valid_loss = 0.0\n",
        "    net.eval()\n",
        "    accs = []\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = net(data)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        accs.append(metrics.acc(output.detach(), target))\n",
        "    return valid_loss, np.mean(accs)\n",
        "    \n",
        "def BPGD(model, criterion, len, image,labels, eps=0.3, alpha=2/255, iters=40,num_ens=1) :\n",
        "    # images = images.to(device)\n",
        "    # labels = labels.to(device)\n",
        "    # loss = nn.CrossEntropyLoss()\n",
        "        \n",
        "    # ori_images = images.data\n",
        "    training_loss = 0.0\n",
        "    accs = []\n",
        "    kl_list = []\n",
        "    for i in range(iters) :  \n",
        "          inputs= image.to(device)\n",
        "          labels=labels.to(device)\n",
        "          outputs = torch.zeros(inputs.shape[0], model.num_classes, num_ens).to(device)\n",
        "          inputs.requires_grad = True\n",
        "          kl = 0.0\n",
        "          for j in range(num_ens):\n",
        "              net_out, _kl = model(inputs)\n",
        "              kl += _kl\n",
        "              outputs[:, :, j] = F.log_softmax(net_out, dim=1)\n",
        "          \n",
        "          kl = kl / num_ens\n",
        "          kl_list.append(kl.item())\n",
        "          log_outputs = utils.logmeanexp(outputs, dim=2)\n",
        "\n",
        "          beta = 1/ len\n",
        "          loss = criterion(log_outputs, labels, kl, beta)\n",
        "          loss.backward()\n",
        "          adv_images = inputs + alpha*inputs.grad.sign()\n",
        "          eta = torch.clamp(adv_images - inputs, min=-eps, max=eps)\n",
        "          images = torch.clamp(inputs + eta, min=0, max=1).detach_()\n",
        "    return images\n",
        "\n",
        "def test_attack_freq(freq,dataset,test_loader,inputs,outputs,attack):\n",
        "    # n_epochs = cfg.n_epochs\n",
        "    attack_dir=f'adv_data/frequentist/{attack}'\n",
        "    battack_dir=f'adv_data/bayesian/{attack}'\n",
        "\n",
        "    if not os.path.exists(attack_dir):\n",
        "      os.makedirs(attack_dir)\n",
        "    if not os.path.exists(battack_dir):\n",
        "      os.makedirs(battack_dir)\n",
        "    lr = cfg.lr\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    dict={}\n",
        "    \n",
        "    for model in freq:\n",
        "        fmodel = getFModel(model, inputs, outputs).to(device)\n",
        "        ckpt_name = f'checkpoints/{dataset}/frequentist/model_{model}.pt'\n",
        "        fmodel.load_state_dict(torch.load(ckpt_name))\n",
        "        fmodel = fmodel.eval().cuda()\n",
        "        test_loss, test_acc = test_model(fmodel, criterion, test_loader)\n",
        "        # for step in range(0,100,10):\n",
        "        # print(\"freq - \",model)\n",
        "        pgdstep=[]\n",
        "        accuracies=[]\n",
        "        for eps in range(0,11,1): \n",
        "            epsilon=np.round(eps*0.05, 2)\n",
        "            badv_dir=f'adv_data/frequentist/{attack}/{dataset}_{model}_{epsilon}_{attack}.pt.tar'\n",
        "            if attack=='PGD':\n",
        "              atk = PGD(fmodel, eps=epsilon, alpha=lr, steps=20)\n",
        "            if attack=='FGSM':\n",
        "              atk= FGSM(fmodel, eps=epsilon)\n",
        "            if attack=='BIM':\n",
        "              atk = BIM(fmodel, eps=epsilon, alpha=lr, steps=20)\n",
        "            if attack=='PGDL2':\n",
        "              atk = PGDL2(fmodel, eps=epsilon, alpha=15/255, steps=10, random_start=False)\n",
        "            atk.set_return_type('int') # Save as integer.\n",
        "            adv_dir=f'adv_data/bayesian/{attack}/{dataset}_{model}_{epsilon}_{attack}.pt.tar'\n",
        "            if not os.path.exists(adv_dir):\n",
        "              atk.save(data_loader=test_loader, save_path=adv_dir, verbose=False)\n",
        "              atk.save(data_loader=test_loader, save_path=badv_dir, verbose=False)\n",
        "            # test_loss = test_loss/len(test_loader.dataset)\n",
        "            \n",
        "            adv_images, adv_labels = torch.load(adv_dir)\n",
        "            adv_data = TensorDataset(adv_images.float()/255, adv_labels)\n",
        "            adv_loader = DataLoader(adv_data, batch_size=cfg.batch_size, shuffle=False)\n",
        "            adv_loss, adv_acc = test_model(fmodel, criterion, adv_loader)\n",
        "            pgdstep.append(epsilon)\n",
        "            accuracies.append(adv_acc)\n",
        "            # print(\"Step\", step, \"Accuracy\", adv_acc)\n",
        "            \n",
        "        dict[model]={\"accu\":accuracies, 'epsilon':pgdstep}\n",
        "    return dict       \n",
        "def test_attack_bayes(bay,dataset,test_loader,inputs,outputs,attack):\n",
        "    layer_type = cfg2.layer_type\n",
        "    activation_type = cfg2.activation_type\n",
        "    priors = cfg2.priors\n",
        "    criterion = metrics.ELBO(len(test_loader)).to(device)\n",
        "\n",
        "    train_ens = cfg2.train_ens\n",
        "    valid_ens = cfg2.valid_ens\n",
        "    n_epochs = cfg2.n_epochs\n",
        "    lr_start = cfg2.lr_start\n",
        "    num_workers = cfg2.num_workers\n",
        "    valid_size = cfg2.valid_size\n",
        "    batch_size = cfg2.batch_size\n",
        "    beta_type = cfg2.beta_type\n",
        "    \n",
        "    # valid_size = cfg2.valid_size\n",
        "    # batch_size = cfg2.batch_size\n",
        "    \n",
        "    # trainset, testset, inputs, outputs = data.getDataset(dataset)\n",
        "    # train_loader, valid_loader, test_loader = data.getDataloader(\n",
        "    #         trainset, testset, valid_size, batch_size, num_workers)\n",
        "    dict={}\n",
        "    for model in bay:\n",
        "        # print(\"Bay - \",model)\n",
        "        ckpt_name = f'checkpoints/{dataset}/bayesian/model_{model[1:]}_{layer_type}_{activation_type}.pt'\n",
        "        # print(ckpt_name)\n",
        "        bmodel = getBModel(model[1:], inputs, outputs, priors, layer_type, activation_type).to(device)\n",
        "    \n",
        "        # ckpt_dir = f'checkpoints/{dataset}/bayesian'\n",
        "        bmodel.load_state_dict(torch.load(ckpt_name))\n",
        "        bmodel = bmodel.eval().cuda()\n",
        "        pgdstep=[]\n",
        "        accuracies=[] \n",
        "        # for step in [40]: \n",
        "        for eps in range(0,11,1):  \n",
        "            # adv_dir=f'adv_data/frequentist/{dataset}_{model[1:]}_{step}_PGD.pt'\n",
        "            epsilon=np.round(eps*0.05,2)\n",
        "            adv_dir=f'adv_data/bayesian/{attack}/{dataset}_{model[1:]}_{epsilon}_{attack}.pt.tar'\n",
        "            if os.path.exists(adv_dir):\n",
        "            # atk.save(data_loader=test_loader, save_path=adv_dir, verbose=True)\n",
        "            # test_loss = test_loss/len(test_loader.dataset)\n",
        "              adv_images, adv_labels = torch.load(adv_dir)\n",
        "              adv_data = TensorDataset(adv_images.float()/255, adv_labels)\n",
        "              adv_loader = DataLoader(adv_data, batch_size=cfg.batch_size, shuffle=False)\n",
        "              adv_loss, adv_acc =  validate_model(bmodel, criterion, adv_loader, num_ens=valid_ens, beta_type=beta_type)\n",
        "              pgdstep.append(epsilon)\n",
        "              accuracies.append(adv_acc)\n",
        "            # print(\"Step\", step, \"Accuracy\", adv_acc)\n",
        "        dict[model]={\"accu\":accuracies, 'epsilon':pgdstep}\n",
        "    return dict\n",
        "        #     correct = 0\n",
        "        #     total = 0\n",
        "        #     for images, labels in test_loader:\n",
        "        #         # model2.train()\n",
        "        #         # btest_loss=0.0\n",
        "        #         # btest_accs=[]\n",
        "        #         # adv_images = BPGD(model2,criterion,len(test_loader),images,labels, eps=0.03, alpha=cfg.lr, iters=step)\n",
        "        #         # labels = labels.to(device)\n",
        "        #         adv_images, adv_labels = torch.load(adv_dir)\n",
        "        #         adv_data = TensorDataset(adv_images.float()/255, adv_labels)\n",
        "        #         adv_loader = DataLoader(adv_data, batch_size=cfg.batch_size, shuffle=False)\n",
        "        #         adv_loss, adv_acc = test_model(fmodel, criterion, adv_loader)\n",
        "        #         pgdstep.append(step)\n",
        "        #         accuracies.append(adv_acc)\n",
        "        #         print(\"Step\", step, \"Accuracy\". adv_acc)\n",
        "        #         # outputs = torch.zeros(adv_images.shape[0], model2.num_classes, 1).to(device)\n",
        "        #         # kl = 0.0\n",
        "        #         # for j in range(1):\n",
        "        #         #     net_out, _kl = model2(adv_images)\n",
        "        #         #     kl += _kl\n",
        "        #         #     outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n",
        "            \n",
        "        #         # log_outputs = utils.logmeanexp(outputs, dim=2)\n",
        "            \n",
        "        #         # criterion = metrics.ELBO(len(test_loader)).to(device)\n",
        "                \n",
        "        #         # beta = 1/ len(test_loader)\n",
        "        #         # btest_loss += criterion(log_outputs, labels, kl, beta).item()\n",
        "        #         # btest_accs.append(metrics.acc(log_outputs, labels))\n",
        "            \n",
        "        #     pgdstep.append(step)\n",
        "        #     accuracies.append(np.mean(btest_accs))\n",
        "        # dict[model]={\"accu\":accuracies, 'steps':pgdstep}\n",
        "           \n",
        "\n",
        "def plot_PGD(dic_1,dic_2,attack,dataset):\n",
        "    \n",
        "    fmodels= dic_1.keys() \n",
        "    bmodels= dic_2.keys()\n",
        "    for f in fmodels:\n",
        "        dic=dic_1[f]\n",
        "        x=dic['epsilon']\n",
        "        y=dic['accu']\n",
        "        # print(f,y)\n",
        "        plt.plot(x, y, label=f, marker='o')\n",
        "        \n",
        "    for b in bmodels:\n",
        "        dic=dic_2[b]\n",
        "        x=dic['epsilon']\n",
        "        y=dic['accu']\n",
        "        plt.plot(x, y, label=b, marker='v')\n",
        "    title=f'{dataset} - {attack} epsilon'\n",
        "    save=f'att_figure/{dataset}_{attack}_epsilon.png'\n",
        "    plt.title(title)\n",
        "    plt.legend(loc='upper right', frameon=False)\n",
        "    plt.xlabel('Epsilon')\n",
        "    plt.ylabel('Test Accuracy')\n",
        "    plt.savefig(save)\n",
        "        \n",
        "def test(dataset,attack):\n",
        "    freq=[]\n",
        "    bay=[]\n",
        "    layer_type = cfg2.layer_type\n",
        "    activation_type = cfg2.activation_type\n",
        "    for model in ['alexnet','lenet']:\n",
        "        fckpt_name = f'checkpoints/{dataset}/frequentist/model_{model}.pt'\n",
        "        bckpt_name = f'checkpoints/{dataset}/bayesian/model_{model}_{layer_type}_{activation_type}.pt'\n",
        "        if os.path.exists(fckpt_name):\n",
        "            freq.append(model)\n",
        "        if os.path.exists(bckpt_name):\n",
        "            bay.append('B'+model)\n",
        "    \n",
        "    valid_size = cfg.valid_size\n",
        "    batch_size = cfg.batch_size\n",
        "    num_workers = cfg.num_workers\n",
        "    \n",
        "    trainset, testset, inputs, outputs = data.getDataset(dataset,'alexnet')\n",
        "    train_loader, valid_loader, test_loader = data.getDataloader(trainset, testset, valid_size, batch_size, num_workers)\n",
        "    \n",
        "\n",
        "    dict_1 = dict_2 = {}\n",
        "    dict_1=test_attack_freq(freq,dataset,test_loader,inputs,outputs,attack)\n",
        "    dict_2=test_attack_bayes(bay,dataset,test_loader,inputs,outputs,attack)\n",
        "    plot_PGD(dict_1,dict_2,attack,dataset)\n",
        "\n",
        "    # if attack=='FGSM':\n",
        "    #     dict_1 = dict_2 = {}\n",
        "    #     dict_1=test_attack_PGD_freq(freq,dataset,test_loader,inputs,outputs)\n",
        "    #     dict_2=test_attack_PGD_bayes(bay,dataset,test_loader,inputs,outputs)\n",
        "    #     plot_PGD(dict_1,dict_2)\n",
        "    \n",
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser(description = \"Test Gradient-Based attack\")\n",
        "#     parser.add_argument('--dataset', default='MNIST', type=str, help='dataset = [MNIST/CIFAR10]')\n",
        "#     parser.add_argument('--attack',default='PGD', type=str, help='attack = [PGD/FGSM]')\n",
        "#     args = parser.parse_args()\n",
        "    \n",
        "# test('MNIST','FGSM')\n",
        "test('MNIST','PGD')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "5fMwIyXYTZpi",
        "outputId": "c95e7dd9-c693-46f7-f175-9ab027416d73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "import os \n",
        "from torch.nn import functional as F\n",
        "\n",
        "from models.BayesianModels.Bayesian3Conv3FC import BBB3Conv3FC\n",
        "from models.BayesianModels.BayesianAlexNet import BBBAlexNet\n",
        "from models.BayesianModels.BayesianLeNet import BBBLeNet\n",
        "from models.NonBayesianModels.AlexNet import AlexNet\n",
        "from models.NonBayesianModels.LeNet import LeNet\n",
        "from models.NonBayesianModels.ThreeConvThreeFC import ThreeConvThreeFC\n",
        "\n",
        "import os.path\n",
        "import data\n",
        "import utils\n",
        "import metrics\n",
        "\n",
        "import config_bayesian as cfg2\n",
        "import config_frequentist as cfg\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def getBModel(net_type, inputs, outputs, priors, layer_type, activation_type):\n",
        "    if (net_type == 'lenet'):\n",
        "        return BBBLeNet(outputs, inputs, priors, layer_type, activation_type)\n",
        "    elif (net_type == 'alexnet'):\n",
        "        return BBBAlexNet(outputs, inputs, priors, layer_type, activation_type)\n",
        "    elif (net_type == '3conv3fc'):\n",
        "        return BBB3Conv3FC(outputs, inputs, priors, layer_type, activation_type)\n",
        "    else:\n",
        "        raise ValueError('Network should be either [LeNet / AlexNet / 3Conv3FC')\n",
        "\n",
        "def getFModel(net_type, inputs, outputs):\n",
        "    if (net_type == 'lenet'):\n",
        "        return LeNet(outputs, inputs)\n",
        "    elif (net_type == 'alexnet'):\n",
        "        return AlexNet(outputs, inputs)\n",
        "    elif (net_type == '3conv3fc'):\n",
        "        return ThreeConvThreeFC(outputs, inputs)\n",
        "    else:\n",
        "        raise ValueError('Network should be either [LeNet / AlexNet / 3Conv3FC')\n",
        "\n",
        "def validate_model(net, criterion, validloader, num_ens=1, beta_type=0.1):\n",
        "    \"\"\"Calculate ensemble accuracy and NLL Loss\"\"\"\n",
        "    net.train()\n",
        "    valid_loss = 0.0\n",
        "    accs = []\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(validloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens).to(device)\n",
        "        kl = 0.0\n",
        "        for j in range(num_ens):\n",
        "            net_out, _kl = net(inputs)\n",
        "            kl += _kl\n",
        "            outputs[:, :, j] = F.log_softmax(net_out, dim=1).data\n",
        "\n",
        "        log_outputs = utils.logmeanexp(outputs, dim=2)\n",
        "\n",
        "        beta = 1/len(validloader)\n",
        "        valid_loss += criterion(log_outputs, labels, kl, beta).item()\n",
        "        accs.append(metrics.acc(log_outputs, labels))\n",
        "\n",
        "    return valid_loss/len(validloader), np.mean(accs)\n",
        "\n",
        "def test_model(net, criterion, test_loader):\n",
        "    valid_loss = 0.0\n",
        "    net.eval()\n",
        "    accs = []\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = net(data)\n",
        "        loss = criterion(output, target)\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        accs.append(metrics.acc(output.detach(), target))\n",
        "    return valid_loss, np.mean(accs)\n",
        "    \n",
        "def BPGD(model, criterion, len, image,labels, eps=0.3, alpha=2/255, iters=40,num_ens=1) :\n",
        "    training_loss = 0.0\n",
        "    accs = []\n",
        "    kl_list = []\n",
        "    for i in range(iters) :  \n",
        "          inputs= image.to(device)\n",
        "          labels=labels.to(device)\n",
        "          outputs = torch.zeros(inputs.shape[0], model.num_classes, num_ens).to(device)\n",
        "          inputs.requires_grad = True\n",
        "          kl = 0.0\n",
        "          for j in range(num_ens):\n",
        "              net_out, _kl = model(inputs)\n",
        "              kl += _kl\n",
        "              outputs[:, :, j] = F.log_softmax(net_out, dim=1)\n",
        "          \n",
        "          kl = kl / num_ens\n",
        "          kl_list.append(kl.item())\n",
        "          log_outputs = utils.logmeanexp(outputs, dim=2)\n",
        "\n",
        "          beta = 1/ len\n",
        "          loss = criterion(log_outputs, labels, kl, beta)\n",
        "          loss.backward()\n",
        "          adv_images = inputs + alpha*inputs.grad.sign()\n",
        "          eta = torch.clamp(adv_images - inputs, min=-eps, max=eps)\n",
        "          images = torch.clamp(inputs + eta, min=0, max=1).detach_()\n",
        "    return images\n",
        "\n",
        "def test_attack_freq(freq,dataset,test_loader,inputs,outputs):\n",
        "  \n",
        "    lr = cfg.lr\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    dict={}\n",
        "    \n",
        "    for model in freq:\n",
        "        fmodel = getFModel(model, inputs, outputs).to(device)\n",
        "        ckpt_name = f'checkpoints/{dataset}/frequentist/model_{model}.pt'\n",
        "        fmodel.load_state_dict(torch.load(ckpt_name))\n",
        "        fmodel = fmodel.eval().cuda()\n",
        "        test_loss, test_acc = test_model(fmodel, criterion, test_loader)\n",
        "        dict[model]={\"accu\":test_acc, 'model':model}\n",
        "    return dict       \n",
        "def test_attack_bayes(bay,dataset,test_loader,inputs,outputs):\n",
        "    layer_type = cfg2.layer_type\n",
        "    activation_type = cfg2.activation_type\n",
        "    priors = cfg2.priors\n",
        "    criterion = metrics.ELBO(len(test_loader)).to(device)\n",
        "\n",
        "    train_ens = cfg2.train_ens\n",
        "    valid_ens = cfg2.valid_ens\n",
        "    n_epochs = cfg2.n_epochs\n",
        "    lr_start = cfg2.lr_start\n",
        "    num_workers = cfg2.num_workers\n",
        "    valid_size = cfg2.valid_size\n",
        "    batch_size = cfg2.batch_size\n",
        "    beta_type = cfg2.beta_type\n",
        "  \n",
        "    dict={}\n",
        "    for model in bay:\n",
        "        ckpt_name = f'checkpoints/{dataset}/bayesian/model_{model[1:]}_{layer_type}_{activation_type}.pt'\n",
        "        bmodel = getBModel(model[1:], inputs, outputs, priors, layer_type, activation_type).to(device)\n",
        "    \n",
        "        bmodel.load_state_dict(torch.load(ckpt_name))\n",
        "        bmodel = bmodel.eval().cuda()\n",
        "        test_loss, test_acc =validate_model(bmodel, criterion, test_loader, num_ens=valid_ens, beta_type=beta_type) \n",
        "        dict[model]={\"accu\":test_acc, 'model':model}\n",
        "    return dict\n",
        "\n",
        "def plot_PGD(dic_1,dic_2,dataset):\n",
        "    fig = plt.figure(figsize = (10, 5))\n",
        "    fmodels= dic_1.keys() \n",
        "    bmodels= dic_2.keys()\n",
        "    for f in fmodels:\n",
        "        dic=dic_1[f]\n",
        "        x=dic['model']\n",
        "        y=dic['accu']\n",
        "        # print(f,y)\n",
        "        plt.bar(x, y, width = 0.4)\n",
        "        \n",
        "    for b in bmodels:\n",
        "        dic=dic_2[b]\n",
        "        x=dic['model']\n",
        "        y=dic['accu']\n",
        "        plt.bar(x, y, width = 0.4)\n",
        "\n",
        "    title=f'{dataset} - Test Accuracies'\n",
        "    save=f'att_figure/{dataset}_Test_Accuracies.png'\n",
        "    plt.xlabel(\"Model\")\n",
        "    plt.ylabel(\"Test Accuracy\")\n",
        "    plt.title(title)\n",
        "    # plt.show()\n",
        "    plt.savefig(save)\n",
        "        \n",
        "def test(dataset):\n",
        "    freq=[]\n",
        "    bay=[]\n",
        "    layer_type = cfg2.layer_type\n",
        "    activation_type = cfg2.activation_type\n",
        "    for model in ['alexnet','lenet','3conv3fc']:\n",
        "        fckpt_name = f'checkpoints/{dataset}/frequentist/model_{model}.pt'\n",
        "        bckpt_name = f'checkpoints/{dataset}/bayesian/model_{model}_{layer_type}_{activation_type}.pt'\n",
        "        if os.path.exists(fckpt_name):\n",
        "            freq.append(model)\n",
        "        if os.path.exists(bckpt_name):\n",
        "            bay.append('B'+model)\n",
        "    \n",
        "    valid_size = cfg.valid_size\n",
        "    batch_size = cfg.batch_size\n",
        "    num_workers = cfg.num_workers\n",
        "    \n",
        "    trainset, testset, inputs, outputs = data.getDataset(dataset,'alexnet')\n",
        "    train_loader, valid_loader, test_loader = data.getDataloader(trainset, testset, valid_size, batch_size, num_workers)    \n",
        "\n",
        "    dict_1 = dict_2 = {}\n",
        "    dict_1=test_attack_freq(freq,dataset,test_loader,inputs,outputs)\n",
        "    dict_2=test_attack_bayes(bay,dataset,test_loader,inputs,outputs)\n",
        "    plot_PGD(dict_1,dict_2,dataset)\n",
        "    \n",
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser(description = \"Test Gradient-Based attack\")\n",
        "#     parser.add_argument('--dataset', default='MNIST', type=str, help='dataset = [MNIST/CIFAR10]')\n",
        "#     parser.add_argument('--attack',default='PGD', type=str, help='attack = [PGD/FGSM]')\n",
        "#     args = parser.parse_args()\n",
        "    \n",
        "# test('MNIST','FGSM')\n",
        "test('CIFAR10')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdDWcGjoGRp-",
        "outputId": "2a7b9695-ddf5-4a71-f922-cab2325fa8fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameters initialized\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "!python test_acc.py --dataset MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNwVeoOJZNwA",
        "outputId": "d67766e7-f5ef-4752-d0ce-2498d1acd8ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.6.5)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1             [-1, 64, 8, 8]           7,808\n",
            "              ReLU-2             [-1, 64, 8, 8]               0\n",
            "              ReLU-3             [-1, 64, 8, 8]               0\n",
            "           Dropout-4             [-1, 64, 8, 8]               0\n",
            "         MaxPool2d-5             [-1, 64, 4, 4]               0\n",
            "            Conv2d-6            [-1, 192, 4, 4]         307,392\n",
            "              ReLU-7            [-1, 192, 4, 4]               0\n",
            "              ReLU-8            [-1, 192, 4, 4]               0\n",
            "         MaxPool2d-9            [-1, 192, 2, 2]               0\n",
            "           Conv2d-10            [-1, 384, 2, 2]         663,936\n",
            "             ReLU-11            [-1, 384, 2, 2]               0\n",
            "             ReLU-12            [-1, 384, 2, 2]               0\n",
            "          Dropout-13            [-1, 384, 2, 2]               0\n",
            "           Conv2d-14            [-1, 256, 2, 2]         884,992\n",
            "             ReLU-15            [-1, 256, 2, 2]               0\n",
            "             ReLU-16            [-1, 256, 2, 2]               0\n",
            "           Conv2d-17            [-1, 256, 2, 2]         590,080\n",
            "             ReLU-18            [-1, 256, 2, 2]               0\n",
            "             ReLU-19            [-1, 256, 2, 2]               0\n",
            "          Dropout-20            [-1, 256, 2, 2]               0\n",
            "        MaxPool2d-21            [-1, 256, 1, 1]               0\n",
            "           Linear-22                   [-1, 10]           2,570\n",
            "================================================================\n",
            "Total params: 2,456,778\n",
            "Trainable params: 2,456,778\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.31\n",
            "Params size (MB): 9.37\n",
            "Estimated Total Size (MB): 9.69\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             156\n",
            "            Conv2d-2           [-1, 16, 10, 10]           2,416\n",
            "            Linear-3                  [-1, 120]          48,120\n",
            "            Linear-4                   [-1, 84]          10,164\n",
            "            Linear-5                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.05\n",
            "Params size (MB): 0.24\n",
            "Estimated Total Size (MB): 0.29\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             832\n",
            "          Softplus-2           [-1, 32, 32, 32]               0\n",
            "          Softplus-3           [-1, 32, 32, 32]               0\n",
            "          Softplus-4           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-5           [-1, 32, 15, 15]               0\n",
            "            Conv2d-6           [-1, 64, 15, 15]          51,264\n",
            "          Softplus-7           [-1, 64, 15, 15]               0\n",
            "          Softplus-8           [-1, 64, 15, 15]               0\n",
            "          Softplus-9           [-1, 64, 15, 15]               0\n",
            "        MaxPool2d-10             [-1, 64, 7, 7]               0\n",
            "           Conv2d-11            [-1, 128, 5, 5]         204,928\n",
            "         Softplus-12            [-1, 128, 5, 5]               0\n",
            "         Softplus-13            [-1, 128, 5, 5]               0\n",
            "         Softplus-14            [-1, 128, 5, 5]               0\n",
            "        MaxPool2d-15            [-1, 128, 2, 2]               0\n",
            "     FlattenLayer-16                  [-1, 512]               0\n",
            "           Linear-17                 [-1, 1000]         513,000\n",
            "         Softplus-18                 [-1, 1000]               0\n",
            "         Softplus-19                 [-1, 1000]               0\n",
            "         Softplus-20                 [-1, 1000]               0\n",
            "           Linear-21                 [-1, 1000]       1,001,000\n",
            "         Softplus-22                 [-1, 1000]               0\n",
            "         Softplus-23                 [-1, 1000]               0\n",
            "         Softplus-24                 [-1, 1000]               0\n",
            "           Linear-25                   [-1, 10]          10,010\n",
            "================================================================\n",
            "Total params: 1,781,034\n",
            "Trainable params: 1,781,034\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.68\n",
            "Params size (MB): 6.79\n",
            "Estimated Total Size (MB): 8.48\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "import os \n",
        "from torchsummary import summary\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def getBModel(net_type, inputs, outputs, priors, layer_type, activation_type):\n",
        "    if (net_type == 'lenet'):\n",
        "        return BBBLeNet(outputs, inputs, priors, layer_type, activation_type)\n",
        "    elif (net_type == 'alexnet'):\n",
        "        return BBBAlexNet(outputs, inputs, priors, layer_type, activation_type)\n",
        "    elif (net_type == '3conv3fc'):\n",
        "        return BBB3Conv3FC(outputs, inputs, priors, layer_type, activation_type)\n",
        "    else:\n",
        "        raise ValueError('Network should be either [LeNet / AlexNet / 3Conv3FC')\n",
        "\n",
        "def getFModel(net_type, inputs, outputs):\n",
        "    if (net_type == 'lenet'):\n",
        "        return LeNet(outputs, inputs)\n",
        "    elif (net_type == 'alexnet'):\n",
        "        return AlexNet(outputs, inputs)\n",
        "    elif (net_type == '3conv3fc'):\n",
        "        return ThreeConvThreeFC(outputs, inputs)\n",
        "    else:\n",
        "        raise ValueError('Network should be either [LeNet / AlexNet / 3Conv3FC')\n",
        "\n",
        "for model in ['alexnet','lenet','3conv3fc']:\n",
        "        freq=[]\n",
        "        bay=[]\n",
        "        layer_type = cfg2.layer_type\n",
        "        activation_type = cfg2.activation_type\n",
        "        fckpt_name = f'checkpoints/CIFAR10/frequentist/model_{model}.pt'\n",
        "        bckpt_name = f'checkpoints/CIFAR10/bayesian/model_{model}_lrt_{activation_type}.pt'\n",
        "        if os.path.exists(fckpt_name):\n",
        "            freq.append(model)\n",
        "        if os.path.exists(bckpt_name):\n",
        "            bay.append('B'+model)\n",
        "        # for model in freq:\n",
        "        #   ckpt_name = f'checkpoints/CIFAR10/frequentist/model_{model}.pt'\n",
        "        #   fmodel = getFModel(model, 3, 10).to(device)\n",
        "        #   fmodel.load_state_dict(torch.load(ckpt_name))\n",
        "        #   print(fmodel)\n",
        "        layer_type = cfg2.layer_type\n",
        "        activation_type = cfg2.activation_type\n",
        "        priors = cfg2.priors\n",
        "        criterion = metrics.ELBO(10).to(device)\n",
        "\n",
        "        train_ens = cfg2.train_ens\n",
        "        valid_ens = cfg2.valid_ens\n",
        "        n_epochs = cfg2.n_epochs\n",
        "        lr_start = cfg2.lr_start\n",
        "        num_workers = cfg2.num_workers\n",
        "        valid_size = cfg2.valid_size\n",
        "        batch_size = cfg2.batch_size\n",
        "        beta_type = cfg2.beta_type\n",
        "        \n",
        "        # valid_size = cfg2.valid_size\n",
        "        # batch_size = cfg2.batch_size\n",
        "        \n",
        "        # trainset, testset, inputs, outputs = data.getDataset(dataset)\n",
        "        # train_loader, valid_loader, test_loader = data.getDataloader(\n",
        "        #         trainset, testset, valid_size, batch_size, num_workers)\n",
        "        dict={}\n",
        "        for model in freq:\n",
        "            fmodel = getFModel(model, 1, 10).to(device)\n",
        "            ckpt_name = f'checkpoints/MNIST/frequentist/model_{model}.pt'\n",
        "            fmodel.load_state_dict(torch.load(ckpt_name))\n",
        "            summary(fmodel, (1, 32, 32))\n",
        "        # print(bay)\n",
        "        # for model in ['lenet']:\n",
        "        #     # print(\"Bay - \",model)\n",
        "        #     ckpt_name = f'checkpoints/MNIST/bayesian/model_{model[1:]}_lrt_{activation_type}.pt'\n",
        "        #     # print(ckpt_name)\n",
        "        #     bmodel = getBModel(model[1:], 3, 10, priors, layer_type, activation_type).to(device)\n",
        "        #     batch_size = 256\n",
        "        #     summary(bmodel, input_size=(3, 32, 32))\n",
        "        #     # ckpt_dir = f'checkpoints/{dataset}/bayesian'\n",
        "        #     print(bmodel)\n",
        "            # summary(bmodel, (3, 32, 32))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FVZvy1yyzZO",
        "outputId": "1f3afcfe-11ac-48d0-a045-8531f4dd0e51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[ 1.6892e-34,  0.0000e+00, -9.3279e+35],\n",
              "          [ 4.5853e-41, -8.9325e+35,  4.5853e-41],\n",
              "          [-3.5393e+35,  4.5853e-41, -3.1077e+35]],\n",
              "\n",
              "         [[ 4.5853e-41, -1.6265e+34,  4.5853e-41],\n",
              "          [-1.0999e+34,  4.5853e-41, -1.3268e+34],\n",
              "          [ 4.5853e-41, -3.5962e+35,  4.5853e-41]],\n",
              "\n",
              "         [[-3.5730e+35,  4.5853e-41, -4.6126e+35],\n",
              "          [ 4.5853e-41, -5.2147e+35,  4.5853e-41],\n",
              "          [-3.2041e+35,  4.5853e-41, -1.2473e+34]],\n",
              "\n",
              "         [[ 4.5853e-41, -1.2431e+34,  4.5853e-41],\n",
              "          [-5.4015e+35,  4.5853e-41, -4.3975e+35],\n",
              "          [ 4.5853e-41, -1.2241e+34,  4.5853e-41]]],\n",
              "\n",
              "\n",
              "        [[[-1.3288e+34,  4.5853e-41, -9.9751e+35],\n",
              "          [ 4.5853e-41, -5.3159e+35,  4.5853e-41],\n",
              "          [-3.5546e+35,  4.5853e-41, -1.2071e+34]],\n",
              "\n",
              "         [[ 4.5853e-41, -3.5541e+35,  4.5853e-41],\n",
              "          [-3.5654e+35,  4.5853e-41, -3.5378e+35],\n",
              "          [ 4.5853e-41, -5.1207e+35,  4.5853e-41]],\n",
              "\n",
              "         [[-3.5652e+35,  4.5853e-41, -5.0381e+35],\n",
              "          [ 4.5853e-41, -5.1334e+35,  4.5853e-41],\n",
              "          [-5.0378e+35,  4.5853e-41, -3.3146e+35]],\n",
              "\n",
              "         [[ 4.5853e-41, -5.1197e+35,  4.5853e-41],\n",
              "          [-3.4037e+35,  4.5853e-41, -3.7056e+35],\n",
              "          [ 4.5853e-41, -3.6564e+35,  4.5853e-41]]],\n",
              "\n",
              "\n",
              "        [[[-3.2013e+35,  4.5853e-41, -3.3992e+35],\n",
              "          [ 4.5853e-41, -3.4210e+35,  4.5853e-41],\n",
              "          [-3.4213e+35,  4.5853e-41, -4.4468e+35]],\n",
              "\n",
              "         [[ 4.5853e-41, -8.7810e+35,  4.5853e-41],\n",
              "          [-1.1150e+34,  4.5853e-41, -9.1549e+35],\n",
              "          [ 4.5853e-41, -4.4833e+35,  4.5853e-41]],\n",
              "\n",
              "         [[-4.4846e+35,  4.5853e-41, -4.3007e+35],\n",
              "          [ 4.5853e-41, -1.9923e+30,  4.5853e-41],\n",
              "          [-3.9362e+35,  4.5853e-41, -4.5294e+35]],\n",
              "\n",
              "         [[ 4.5853e-41, -4.2852e+35,  4.5853e-41],\n",
              "          [-4.2848e+35,  4.5853e-41, -3.4175e+35],\n",
              "          [ 4.5853e-41, -4.0999e+35,  4.5853e-41]]],\n",
              "\n",
              "\n",
              "        [[[-9.8846e+35,  4.5853e-41, -1.8650e+30],\n",
              "          [ 4.5853e-41, -4.0251e+35,  4.5853e-41],\n",
              "          [-4.9185e+35,  4.5853e-41, -4.2840e+35]],\n",
              "\n",
              "         [[ 4.5853e-41, -4.0988e+35,  4.5853e-41],\n",
              "          [-4.6949e+35,  4.5853e-41,  9.4102e-02],\n",
              "          [ 4.5853e-41,  9.4469e-02,  4.5853e-41]],\n",
              "\n",
              "         [[ 9.4101e-02,  4.5853e-41,  9.4101e-02],\n",
              "          [ 4.5853e-41,  9.3284e-02,  4.5853e-41],\n",
              "          [ 9.4473e-02,  4.5853e-41, -4.0249e+35]],\n",
              "\n",
              "         [[ 4.5853e-41, -3.5540e+10,  4.5855e-41],\n",
              "          [ 9.1312e-02,  4.5853e-41,  9.1320e-02],\n",
              "          [ 4.5853e-41,  9.4100e-02,  4.5853e-41]]],\n",
              "\n",
              "\n",
              "        [[[ 9.4862e-02,  4.5853e-41,  9.1979e-02],\n",
              "          [ 4.5853e-41,  9.4467e-02,  4.5853e-41],\n",
              "          [ 9.1969e-02,  4.5853e-41,  9.4229e-02]],\n",
              "\n",
              "         [[ 4.5853e-41,  9.4099e-02,  4.5853e-41],\n",
              "          [ 9.4756e-02,  4.5853e-41,  9.1319e-02],\n",
              "          [ 4.5853e-41,  9.4478e-02,  4.5853e-41]],\n",
              "\n",
              "         [[ 9.4102e-02,  4.5853e-41, -3.5622e+10],\n",
              "          [ 4.5855e-41,  9.4869e-02,  4.5853e-41],\n",
              "          [ 8.8186e-02,  4.5853e-41,  9.2727e-02]],\n",
              "\n",
              "         [[ 4.5853e-41,  9.0745e-02,  4.5853e-41],\n",
              "          [ 8.8008e-02,  4.5853e-41,  0.0000e+00],\n",
              "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]], requires_grad=True)"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.nn import Parameter\n",
        "Parameter(torch.Tensor(5,4,*(3,3)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "7a14979e061517ff8db1e3d4e5b2588024b75be1831ccaffba4da2ed3a779201"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}